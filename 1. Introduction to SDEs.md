# Introduction to Stochastic Differential Equations (SDEs)

## Course Overview
- **Grading**:
  - Project (different topics in groups): 45%
  - Exam (testing theoretical concepts): 45%
  - Homework: 10%
- **Schedule**: Will be posted later.

## Why Study SDEs?
SDEs are essential in modeling systems with **uncertainty**, such as:
- Weather forecasting
- Chemical reactions
- Trajectories of vessels (aerospace)
- Any system where parameters or equations are uncertain (e.g., fluctuating source terms).

## Analytical vs. Numerical Methods
- **Analytical solutions**: Rare, mostly for linear SDEs.
- **Numerical methods**: Required for most real-world applications.

## General Form of SDEs
$$
\frac{dx}{dt} = f(x,t) + g(x,t) \left[ \int (k_0)^2 \, dx \right]
$$
- **Noise term**: Deterministic function.
- **Integral form**: For highly irregular functions, we need to understand properties of random variables (distribution, averages, variances).

---

## Recap: Random Variables
### Discrete Random Variables
- **Example**: Throwing a dice.
  - Outcomes: $X = \{1, 2, 3, 4, 5, 6\}$
  - Probabilities: $p_i = \mathbb{P}[X = i]$, with $\sum_{i=1}^{6} p_i = 1$.

- **Bernoulli RV**: $X$ takes values 0 or 1 with probabilities $1-p$ and $p$.
  - **Binomial distribution**: For $N$ coin tosses, $P(X = i) = \binom{N}{i} p^i (1-p)^{N-i}$.

### Continuous Random Variables
- **Probability density function (PDF)**: $\mathbb{P}(a \leq X \leq b) = \int_{a}^{b} p(x) \, dx$.
  - Properties:
    1. $p(x) \geq 0$
    2. $\int_{-\infty}^{\infty} p(x) \, dx = 1$
  - **Examples**:
    - Uniform distribution: $p(x) = \frac{1}{b-a}$ for $x \in (a, b)$.
    - Exponential distribution: $p(x) = \lambda e^{-\lambda x}$.

---

## Expectation and Variance
### Expectation Value
- **Definition**: $E[X] = \sum_{i=1}^{N} x_i p_i$ (discrete) or $E[X] = \int x p(x) \, dx$ (continuous).
- **Properties**:
  - Linearity: $E[aX] = aE[X]$, $E[X + Y] = E[X] + E[Y]$.
  - For a function $h$: $E[h(X)] = \sum h(x_i) p_i$ or $\int h(x) p(x) \, dx$.

### Variance
- **Definition**: $\text{Var}[X] = E[(X - E[X])^2] = E[X^2] - (E[X])^2$.
- **Properties**:
  - $\text{Var}[aX] = a^2 \text{Var}[X]$.
  - For Bernoulli RV: $\text{Var}[X] = p(1-p)$.

---

## Inequalities
1. **Jensen’s Inequality**: For convex $f$, $E[f(X)] \geq f(E[X])$.
2. **Markov’s Inequality**: $P( X| > a) \leq \frac{E[|X|]}{a}$.
3. **Chebyshev’s Inequality**: $P(|X - E[X]| > a) \leq \frac{\text{Var}[X]}{a^2}$.

---

## Independence
- **Definition**: $X$ and $Y$ are independent if $E[g(X)h(Y)] = E[g(X)]E[h(Y)]$ for all functions $g, h$.
- **Normal RVs**: $X \sim N(\mu, \sigma^2)$. Independence $\iff E[XY] = E[X]E[Y]$.

---

## Central Limit Theorem (CLT)
- For i.i.d. RVs $X_i$ with mean $\mu$ and variance $\sigma^2$:
$$
  \frac{\sum_{i=1}^N X_i - N\mu}{\sigma \sqrt{N}} \xrightarrow{d} N(0,1) \quad \text{as} \quad N \to \infty
$$

---

## Next Topic
- [[2. Wiener Process & Brownian Motion]]

---
**Tags**: #SDEs #Probability #RandomVariables #CLT #Expectation #Variance